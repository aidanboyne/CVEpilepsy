{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Best Metrics\n",
    "\n",
    "Loops through each set of 15 epochs for all the cross validation cycles and finds the combination of epochs with the best results given a set of metrics and weights for their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path to your diagnostic folder here\n",
    "diagnostics_folder = os.path.join(os.getcwd(),'4fold_diagnostics_text')\n",
    "metrics = ['accuracy', 'auc']\n",
    "weights = {'accuracy': 0.5, 'auc': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_metrics(file_path, required_metrics):\n",
    "    \"\"\"\n",
    "    Extract specified metrics from a diagnostic file.\n",
    "    \n",
    "    `param file_path`: Path to the diagnostic file\n",
    "    `param required_metrics`: List of metrics to extract\n",
    "    `return`: A dictionary with the extracted metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    metric_patterns = {\n",
    "        'precision': r'Precision:\\s+(\\d\\.\\d+)',\n",
    "        'recall': r'Recall:\\s+(\\d\\.\\d+)',\n",
    "        'f1_score': r'F1 Score:\\s+(\\d\\.\\d+)',\n",
    "        'accuracy': r'Accuracy:\\s+(\\d\\.\\d+)',\n",
    "        'auc': r'AUC:\\s+(\\d\\.\\d+)'\n",
    "    }\n",
    "    \n",
    "    for metric in required_metrics:\n",
    "        if metric in metric_patterns:\n",
    "            match = re.search(metric_patterns[metric], content)\n",
    "            if match:\n",
    "                metrics[metric] = float(match.group(1))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_weighted_average(metrics, weights):\n",
    "    \"\"\"\n",
    "    Calculate the weighted average of the metrics.\n",
    "    \n",
    "    `param metrics`: Dictionary of metrics\n",
    "    `param weights`: Dictionary of weights for each metric\n",
    "    `return`: Weighted average score\n",
    "    \"\"\"\n",
    "    weighted_sum = 0\n",
    "    total_weight = sum(weights.values())\n",
    "    \n",
    "    for metric, weight in weights.items():\n",
    "        if metric in metrics:\n",
    "            weighted_sum += metrics[metric] * weight\n",
    "    \n",
    "    return weighted_sum / total_weight if total_weight > 0 else 0\n",
    "\n",
    "def average_metrics(metrics_list):\n",
    "    \"\"\"\n",
    "    Calculate the average of each metric across all dictionaries in the list.\n",
    "    \n",
    "    :param metrics_list: List of dictionaries containing metrics\n",
    "    :return: Dictionary with averaged metrics\n",
    "    \"\"\"\n",
    "    if not metrics_list:\n",
    "        return {}\n",
    "\n",
    "    # Initialize a dictionary to store the sum of each metric\n",
    "    sum_metrics = {}\n",
    "    \n",
    "    # Sum up all the metrics\n",
    "    for metrics in metrics_list:\n",
    "        for key, value in metrics.items():\n",
    "            if key not in sum_metrics:\n",
    "                sum_metrics[key] = 0\n",
    "            sum_metrics[key] += value\n",
    "    \n",
    "    # Calculate the average for each metric\n",
    "    avg_metrics = {key: value / len(metrics_list) for key, value in sum_metrics.items()}\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "def find_best_model(diagnostics_folder, metrics, weights):\n",
    "    \"\"\"\n",
    "    Find the best model based on a weighted average of metrics.\n",
    "    \n",
    "    `param diagnostics_folder`: Folder containing the diagnostic files\n",
    "    `param metrics`: List of metrics to consider\n",
    "    `param weights`: Dictionary of weights for each metric\n",
    "    `return`: The best model's diagnostic file and its weighted average score\n",
    "    \"\"\"\n",
    "    diagnostic_files = sorted(glob.glob(os.path.join(diagnostics_folder, 'diagnostics_*.txt')))\n",
    "    cross_validation_groups = [diagnostic_files[i:i+15] for i in range(0, len(diagnostic_files), 15)]\n",
    "    \n",
    "    best_scores = []\n",
    "    best_metrics_averages = []\n",
    "    best_files = []\n",
    "    \n",
    "    for group in cross_validation_groups:\n",
    "        group_best_score = -np.inf\n",
    "        group_best_metrics = None\n",
    "        group_best_file = None\n",
    "        \n",
    "        for file in group:\n",
    "            metrics_data = extract_metrics(file, metrics)\n",
    "            score = calculate_weighted_average(metrics_data, weights)\n",
    "            \n",
    "            if score > group_best_score:\n",
    "                group_best_score = score\n",
    "                group_best_metrics = metrics_data\n",
    "                group_best_file = file\n",
    "\n",
    "        best_scores.append(group_best_score)\n",
    "        best_metrics_averages.append(group_best_metrics)\n",
    "        best_files.append(group_best_file)\n",
    "\n",
    "    average_best_score = np.average(best_scores)\n",
    "    best_metrics_averages = average_metrics(best_metrics_averages)\n",
    "\n",
    "    return best_files, average_best_score, best_metrics_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model files: ['c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\4fold_diagnostics_text\\\\diagnostics_20240710_121915.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\4fold_diagnostics_text\\\\diagnostics_20240710_130729.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\4fold_diagnostics_text\\\\diagnostics_20240710_135254.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\4fold_diagnostics_text\\\\diagnostics_20240710_150020.txt']\n",
      "Best model score: 0.9617500000000001\n",
      "Best model metrics averages: {'accuracy': 0.9453, 'auc': 0.9782}\n"
     ]
    }
   ],
   "source": [
    "best_model, best_score, best_metric_averages = find_best_model(diagnostics_folder, metrics, weights)\n",
    "print(f\"Best model files: {best_model}\")\n",
    "print(f\"Best model score: {best_score}\")\n",
    "print(f\"Best model metrics averages: {best_metric_averages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u251245\\CVEpilepsy\\4fold_diagnostics_text\\diagnostics_20240710_121915.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\4fold_diagnostics_text\\diagnostics_20240710_130729.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\4fold_diagnostics_text\\diagnostics_20240710_135254.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\4fold_diagnostics_text\\diagnostics_20240710_150020.txt\n"
     ]
    }
   ],
   "source": [
    "for model in best_model:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Anthony's Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_averages(file_path):\n",
    "    \"\"\"\n",
    "    Calculate the averages of all metrics found in a text file.\n",
    "    \n",
    "    :param file_path: Path to the text file containing the metrics\n",
    "    :return: A dictionary with the average values of each metric\n",
    "    \"\"\"\n",
    "    metrics_sum = {}\n",
    "    metrics_count = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Regular expression to find all metrics\n",
    "    pattern = re.compile(r'(\\w+(?: \\w+)*):\\s+(\\d\\.\\d+)')\n",
    "    matches = pattern.findall(content)\n",
    "    \n",
    "    for match in matches:\n",
    "        metric_name = match[0].replace(' ', '_')  # Replace spaces with underscores for consistency\n",
    "        metric_value = float(match[1])\n",
    "        \n",
    "        if metric_name in metrics_sum:\n",
    "            metrics_sum[metric_name] += metric_value\n",
    "            metrics_count[metric_name] += 1\n",
    "        else:\n",
    "            metrics_sum[metric_name] = metric_value\n",
    "            metrics_count[metric_name] = 1\n",
    "    \n",
    "    # Calculate averages\n",
    "    metrics_averages = {metric: metrics_sum[metric] / metrics_count[metric] for metric in metrics_sum}\n",
    "    \n",
    "    return metrics_averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncropped Averages:\n",
      "{'roc_auc': 0.9189666666666668, 'top1_acc': 0.8088791666666665, 'modified_acc': 0.7989062499999998, 'modified_auc': 0.9940958333333333}\n",
      "\n",
      "Cropped Averages:\n",
      "{'roc_auc': 0.9471333333333333, 'top1_acc': 0.8334583333333332, 'modified_acc': 0.8250708333333333, 'modified_auc': 0.9983416666666667}\n"
     ]
    }
   ],
   "source": [
    "file_path_uncropped = 'C:/Users/u251245/CVEpilepsy/anthony_results_i3d_uncropped.txt'\n",
    "file_path_cropped = 'C:/Users/u251245/CVEpilepsy/anthony_results_i3d_cropped.txt'\n",
    "averages_uc = calculate_metrics_averages(file_path_uncropped)\n",
    "averages_c = calculate_metrics_averages(file_path_cropped)\n",
    "print(f'Uncropped Averages:\\n{averages_uc}\\n')\n",
    "print(f'Cropped Averages:\\n{averages_c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
