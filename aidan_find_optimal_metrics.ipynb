{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "1. Find best epoch from each CV cycle with `find_best_model`\n",
    "2. Calculate old results from Anthony's runs\n",
    "3. Calculate and plot best metrics from `aidan_test.py`\n",
    "\n",
    "### Find Best Metrics\n",
    "\n",
    "Loops through each set of 15 epochs for all the cross validation cycles and finds the combination of epochs with the best results given a set of metrics and weights for their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path to your diagnostic folder here\n",
    "# diagnostics_folder = os.path.join(os.getcwd(),'diagnostic_save_path')\n",
    "diagnostic_folder = os.path.join(os.getcwd(), 'archived/train_7.12/diagnostic_save_path')\n",
    "metrics = ['accuracy', 'auc']\n",
    "weights = {'accuracy': 0.5, 'auc': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_metrics(file_path, required_metrics):\n",
    "    \"\"\"\n",
    "    Extract specified metrics from a diagnostic file.\n",
    "    \n",
    "    `param file_path`: Path to the diagnostic file\n",
    "    `param required_metrics`: List of metrics to extract\n",
    "    `return`: A dictionary with the extracted metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    metric_patterns = {\n",
    "        'precision': r'Precision:\\s+(\\d\\.\\d+)',\n",
    "        'recall': r'Recall:\\s+(\\d\\.\\d+)',\n",
    "        'f1_score': r'F1 Score:\\s+(\\d\\.\\d+)',\n",
    "        'accuracy': r'Accuracy:\\s+(\\d\\.\\d+)',\n",
    "        'auc': r'AUC:\\s+(\\d\\.\\d+)'\n",
    "    }\n",
    "    \n",
    "    for metric in required_metrics:\n",
    "        if metric in metric_patterns:\n",
    "            match = re.search(metric_patterns[metric], content)\n",
    "            if match:\n",
    "                metrics[metric] = float(match.group(1))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_weighted_average(metrics, weights):\n",
    "    \"\"\"\n",
    "    Calculate the weighted average of the metrics.\n",
    "    \n",
    "    `param metrics`: Dictionary of metrics\n",
    "    `param weights`: Dictionary of weights for each metric\n",
    "    `return`: Weighted average score\n",
    "    \"\"\"\n",
    "    weighted_sum = 0\n",
    "    total_weight = sum(weights.values())\n",
    "    \n",
    "    for metric, weight in weights.items():\n",
    "        if metric in metrics:\n",
    "            weighted_sum += metrics[metric] * weight\n",
    "    \n",
    "    return weighted_sum / total_weight if total_weight > 0 else 0\n",
    "\n",
    "def average_metrics(metrics_list):\n",
    "    \"\"\"\n",
    "    Calculate the average of each metric across all dictionaries in the list.\n",
    "    \n",
    "    :param metrics_list: List of dictionaries containing metrics\n",
    "    :return: Dictionary with averaged metrics\n",
    "    \"\"\"\n",
    "    if not metrics_list:\n",
    "        return {}\n",
    "\n",
    "    # Initialize a dictionary to store the sum of each metric\n",
    "    sum_metrics = {}\n",
    "    \n",
    "    # Sum up all the metrics\n",
    "    for metrics in metrics_list:\n",
    "        for key, value in metrics.items():\n",
    "            if key not in sum_metrics:\n",
    "                sum_metrics[key] = 0\n",
    "            sum_metrics[key] += value\n",
    "    \n",
    "    # Calculate the average for each metric\n",
    "    avg_metrics = {key: value / len(metrics_list) for key, value in sum_metrics.items()}\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "def find_best_model(diagnostics_folder, metrics, weights):\n",
    "    \"\"\"\n",
    "    Find the best model based on a weighted average of metrics.\n",
    "    \n",
    "    `param diagnostics_folder`: Folder containing the diagnostic files\n",
    "    `param metrics`: List of metrics to consider\n",
    "    `param weights`: Dictionary of weights for each metric\n",
    "    `return`: The best model's diagnostic file and its weighted average score\n",
    "    \"\"\"\n",
    "    diagnostic_files = sorted(glob.glob(os.path.join(diagnostics_folder, 'diagnostics_*.txt')))\n",
    "    cross_validation_groups = [diagnostic_files[i:i+15] for i in range(0, len(diagnostic_files), 15)]\n",
    "    \n",
    "    best_scores = []\n",
    "    best_metrics_averages = []\n",
    "    best_files = []\n",
    "    \n",
    "    for i, group in enumerate(cross_validation_groups):\n",
    "        group_best_score = -np.inf\n",
    "        group_best_metrics = None\n",
    "        group_best_file = None\n",
    "        \n",
    "        for file in group:\n",
    "            metrics_data = extract_metrics(file, metrics)\n",
    "            score = calculate_weighted_average(metrics_data, weights)\n",
    "            \n",
    "            if score > group_best_score:\n",
    "                group_best_score = score\n",
    "                group_best_metrics = metrics_data\n",
    "                group_best_file = file\n",
    "\n",
    "        best_scores.append(group_best_score)\n",
    "        best_metrics_averages.append(group_best_metrics)\n",
    "        best_files.append(group_best_file)\n",
    "        print(f\"Group {i+1} best metrics: {group_best_metrics}\")\n",
    "\n",
    "    average_best_score = np.average(best_scores)\n",
    "    best_metrics_averages = average_metrics(best_metrics_averages)\n",
    "\n",
    "    return best_files, average_best_score, best_metrics_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 best metrics: {'accuracy': 0.8321, 'auc': 0.897}\n",
      "Group 2 best metrics: {'accuracy': 0.7715, 'auc': 0.9046}\n",
      "Group 3 best metrics: {'accuracy': 0.8138, 'auc': 0.8736}\n",
      "Group 4 best metrics: {'accuracy': 0.7517, 'auc': 0.8116}\n",
      "Best model files: ['c:\\\\Users\\\\u251245\\\\CVEpilepsy_remote\\\\diagnostic_save_path\\\\diagnostics_20240805_081912.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy_remote\\\\diagnostic_save_path\\\\diagnostics_20240805_083336.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy_remote\\\\diagnostic_save_path\\\\diagnostics_20240805_091013.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy_remote\\\\diagnostic_save_path\\\\diagnostics_20240805_093814.txt']\n",
      "Best model score: 0.8319875\n",
      "Best model metrics averages: {'accuracy': 0.792275, 'auc': 0.8717}\n"
     ]
    }
   ],
   "source": [
    "best_model, best_score, best_metric_averages = find_best_model(diagnostics_folder, metrics, weights)\n",
    "print(f\"Best model files: {best_model}\")\n",
    "print(f\"Best model score: {best_score}\")\n",
    "print(f\"Best model metrics averages: {best_metric_averages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy models to best_model folder\n",
    "import shutil\n",
    "\n",
    "def copy_files_to_directory(file_paths, destination_directory):\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "    \n",
    "    successful_copies = 0\n",
    "    failed_copies = 0\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            destination_path = os.path.join(destination_directory, file_name)\n",
    "            shutil.copy2(file_path, destination_path)\n",
    "            \n",
    "            print(f\"Successfully copied: {file_path} to {destination_path}\")\n",
    "            successful_copies += 1\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            failed_copies += 1\n",
    "        \n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: {file_path}\")\n",
    "            failed_copies += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {file_path}: {str(e)}\")\n",
    "            failed_copies += 1\n",
    "    \n",
    "    print(f\"\\nCopy operation completed.\")\n",
    "    print(f\"Successfully copied files: {successful_copies}\")\n",
    "    print(f\"Failed copies: {failed_copies}\")\n",
    "\n",
    "target_dir = 'final_results//BEST_UNCROPPED'\n",
    "copy_files_to_directory(['c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_082359.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_090442.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_093045.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_100403.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_104457.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_111236.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_114955.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_123825.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_132544.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_135805.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_150522.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_154259.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_162844.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_172732.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_181703.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_190650.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_194943.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_204528.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_212802.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_221802.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_230233.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240808_235805.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240809_004642.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240809_011814.txt', 'c:\\\\Users\\\\aidan\\\\CVEpilepsy\\\\final_results//LOPO_TRAIN_UNCROPPED\\\\diagnostics_20240809_021946.txt'], os.path.join(os.getcwd(), target_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in best_model:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Anthony's Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_averages(file_path):\n",
    "    \"\"\"\n",
    "    Calculate the averages of all metrics found in a text file.\n",
    "    \n",
    "    :param file_path: Path to the text file containing the metrics\n",
    "    :return: A dictionary with the average values of each metric\n",
    "    \"\"\"\n",
    "    metrics_sum = {}\n",
    "    metrics_count = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Regular expression to find all metrics\n",
    "    pattern = re.compile(r'(\\w+(?: \\w+)*):\\s+(\\d\\.\\d+)')\n",
    "    matches = pattern.findall(content)\n",
    "    \n",
    "    for match in matches:\n",
    "        metric_name = match[0].replace(' ', '_')  # Replace spaces with underscores for consistency\n",
    "        metric_value = float(match[1])\n",
    "        \n",
    "        if metric_name in metrics_sum:\n",
    "            metrics_sum[metric_name] += metric_value\n",
    "            metrics_count[metric_name] += 1\n",
    "        else:\n",
    "            metrics_sum[metric_name] = metric_value\n",
    "            metrics_count[metric_name] = 1\n",
    "    \n",
    "    # Calculate averages\n",
    "    metrics_averages = {metric: metrics_sum[metric] / metrics_count[metric] for metric in metrics_sum}\n",
    "    \n",
    "    return metrics_averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_uncropped = 'C:/Users/u251245/CVEpilepsy/anthony_results_i3d_uncropped.txt'\n",
    "file_path_cropped = 'C:/Users/u251245/CVEpilepsy/anthony_results_i3d_cropped.txt'\n",
    "averages_uc = calculate_metrics_averages(file_path_uncropped)\n",
    "averages_c = calculate_metrics_averages(file_path_cropped)\n",
    "print(f'Uncropped Averages:\\n{averages_uc}\\n')\n",
    "print(f'Cropped Averages:\\n{averages_c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diagnostic_files(save_dir:str=os.path.join(os.getcwd(),'diagnostic_save_path')):\n",
    "    results = []\n",
    "    for r, d, files in os.walk(save_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                results.append(os.path.join(r,file))\n",
    "    return results\n",
    "\n",
    "file_paths = get_diagnostic_files()\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_metrics = ['accuracy', 'auc']\n",
    "results = []\n",
    "video_names_ordered = []\n",
    "for _,_,videos in os.walk(os.path.join(os.getcwd(), 'annotations', 'video_test_annotations')):\n",
    "    for video in videos:\n",
    "        video_names_ordered.append(video.strip('.txt'))\n",
    "for i, file in enumerate(get_diagnostic_files(save_dir=os.path.join(os.getcwd(),'diagnostic_save_path'))):\n",
    "    output = extract_metrics(file, req_metrics)\n",
    "    output['video'] = video_names_ordered[i]\n",
    "    results.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [entry['video'] for entry in results]\n",
    "print(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_metrics(data):\n",
    "    videos = [entry['video'] for entry in data]\n",
    "    accuracies = [entry['accuracy'] for entry in data]\n",
    "    aucs = [entry['auc'] for entry in data]\n",
    "    print(f\"Average accuracy TEST: {np.mean(accuracies):.3f}\\nAverage AUC TEST: {np.mean(aucs):.3f}\\n\")\n",
    "\n",
    "    x = np.arange(len(videos))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy')\n",
    "    rects2 = ax.bar(x + width/2, aucs, width, label='AUC')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_xlabel('Videos')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('AUC and Accuracy by Video')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(videos)\n",
    "    ax.legend()\n",
    "\n",
    "    # Rotate the video labels to fit them better\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
