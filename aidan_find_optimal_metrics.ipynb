{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Best Metrics\n",
    "\n",
    "Loops through each set of 15 epochs for all the cross validation cycles and finds the combination of epochs with the best results given a set of metrics and weights for their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path to your diagnostic folder here\n",
    "diagnostics_folder = os.path.join(os.getcwd(),'diagnostics_TRAIN')\n",
    "metrics = ['accuracy', 'auc']\n",
    "weights = {'accuracy': 0.3, 'auc': 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_metrics(file_path, required_metrics):\n",
    "    \"\"\"\n",
    "    Extract specified metrics from a diagnostic file.\n",
    "    \n",
    "    `param file_path`: Path to the diagnostic file\n",
    "    `param required_metrics`: List of metrics to extract\n",
    "    `return`: A dictionary with the extracted metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    metric_patterns = {\n",
    "        'precision': r'Precision:\\s+(\\d\\.\\d+)',\n",
    "        'recall': r'Recall:\\s+(\\d\\.\\d+)',\n",
    "        'f1_score': r'F1 Score:\\s+(\\d\\.\\d+)',\n",
    "        'accuracy': r'Accuracy:\\s+(\\d\\.\\d+)',\n",
    "        'auc': r'AUC:\\s+(\\d\\.\\d+)'\n",
    "    }\n",
    "    \n",
    "    for metric in required_metrics:\n",
    "        if metric in metric_patterns:\n",
    "            match = re.search(metric_patterns[metric], content)\n",
    "            if match:\n",
    "                metrics[metric] = float(match.group(1))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_weighted_average(metrics, weights):\n",
    "    \"\"\"\n",
    "    Calculate the weighted average of the metrics.\n",
    "    \n",
    "    `param metrics`: Dictionary of metrics\n",
    "    `param weights`: Dictionary of weights for each metric\n",
    "    `return`: Weighted average score\n",
    "    \"\"\"\n",
    "    weighted_sum = 0\n",
    "    total_weight = sum(weights.values())\n",
    "    \n",
    "    for metric, weight in weights.items():\n",
    "        if metric in metrics:\n",
    "            weighted_sum += metrics[metric] * weight\n",
    "    \n",
    "    return weighted_sum / total_weight if total_weight > 0 else 0\n",
    "\n",
    "def find_best_model(diagnostics_folder, metrics, weights):\n",
    "    \"\"\"\n",
    "    Find the best model based on a weighted average of metrics.\n",
    "    \n",
    "    `param diagnostics_folder`: Folder containing the diagnostic files\n",
    "    `param metrics`: List of metrics to consider\n",
    "    `param weights`: Dictionary of weights for each metric\n",
    "    `return`: The best model's diagnostic file and its weighted average score\n",
    "    \"\"\"\n",
    "    diagnostic_files = sorted(glob.glob(os.path.join(diagnostics_folder, 'diagnostics_*.txt')))\n",
    "    cross_validation_groups = [diagnostic_files[i:i+15] for i in range(0, len(diagnostic_files), 15)]\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_group = None\n",
    "    best_metrics_averages = None\n",
    "    \n",
    "    for group in cross_validation_groups:\n",
    "        group_scores = []\n",
    "        group_metrics = {metric: [] for metric in metrics}\n",
    "        \n",
    "        for file in group:\n",
    "            metrics_data = extract_metrics(file, metrics)\n",
    "            score = calculate_weighted_average(metrics_data, weights)\n",
    "            group_scores.append(score)\n",
    "            \n",
    "            for metric in metrics:\n",
    "                if metric in metrics_data:\n",
    "                    group_metrics[metric].append(metrics_data[metric])\n",
    "        \n",
    "        avg_group_score = np.mean(group_scores)\n",
    "        \n",
    "        if avg_group_score > best_score:\n",
    "            best_score = avg_group_score\n",
    "            best_group = group\n",
    "            best_metrics_averages = {metric: np.mean(values) for metric, values in group_metrics.items()}\n",
    "\n",
    "    return best_group, best_score, best_metrics_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model files: ['c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_140118.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_142726.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_145356.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_152020.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_154658.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_161323.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_163956.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_170649.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_173312.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_175919.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_182529.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_185158.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_191810.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_194356.txt', 'c:\\\\Users\\\\u251245\\\\CVEpilepsy\\\\diagnostics_TRAIN\\\\diagnostics_20240621_200935.txt']\n",
      "Best model score: 0.9351253333333334\n",
      "Best model metrics averages: {'accuracy': 0.9603999999999999, 'auc': 0.9242933333333333}\n"
     ]
    }
   ],
   "source": [
    "best_model, best_score, best_metric_averages = find_best_model(diagnostics_folder, metrics, weights)\n",
    "print(f\"Best model files: {best_model}\")\n",
    "print(f\"Best model score: {best_score}\")\n",
    "print(f\"Best model metrics averages: {best_metric_averages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_140118.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_142726.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_145356.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_152020.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_154658.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_161323.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_163956.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_170649.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_173312.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_175919.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_182529.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_185158.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_191810.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_194356.txt\n",
      "c:\\Users\\u251245\\CVEpilepsy\\diagnostics_TRAIN\\diagnostics_20240621_200935.txt\n"
     ]
    }
   ],
   "source": [
    "for model in best_model:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Anthony's Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_averages(file_path):\n",
    "    \"\"\"\n",
    "    Calculate the averages of all metrics found in a text file.\n",
    "    \n",
    "    :param file_path: Path to the text file containing the metrics\n",
    "    :return: A dictionary with the average values of each metric\n",
    "    \"\"\"\n",
    "    metrics_sum = {}\n",
    "    metrics_count = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Regular expression to find all metrics\n",
    "    pattern = re.compile(r'(\\w+(?: \\w+)*):\\s+(\\d\\.\\d+)')\n",
    "    matches = pattern.findall(content)\n",
    "    \n",
    "    for match in matches:\n",
    "        metric_name = match[0].replace(' ', '_')  # Replace spaces with underscores for consistency\n",
    "        metric_value = float(match[1])\n",
    "        \n",
    "        if metric_name in metrics_sum:\n",
    "            metrics_sum[metric_name] += metric_value\n",
    "            metrics_count[metric_name] += 1\n",
    "        else:\n",
    "            metrics_sum[metric_name] = metric_value\n",
    "            metrics_count[metric_name] = 1\n",
    "    \n",
    "    # Calculate averages\n",
    "    metrics_averages = {metric: metrics_sum[metric] / metrics_count[metric] for metric in metrics_sum}\n",
    "    \n",
    "    return metrics_averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncropped Averages:\n",
      "{'roc_auc': 0.9189666666666668, 'top1_acc': 0.8088791666666665, 'modified_acc': 0.7989062499999998, 'modified_auc': 0.9940958333333333}\n",
      "\n",
      "Cropped Averages:\n",
      "{'roc_auc': 0.9471333333333333, 'top1_acc': 0.8334583333333332, 'modified_acc': 0.8250708333333333, 'modified_auc': 0.9983416666666667}\n"
     ]
    }
   ],
   "source": [
    "file_path_uncropped = 'C:/Users/u251245/CVEpilepsy/anthony_results_i3d_uncropped.txt'\n",
    "file_path_cropped = 'C:/Users/u251245/CVEpilepsy/anthony_results_i3d_cropped.txt'\n",
    "averages_uc = calculate_metrics_averages(file_path_uncropped)\n",
    "averages_c = calculate_metrics_averages(file_path_cropped)\n",
    "print(f'Uncropped Averages:\\n{averages_uc}\\n')\n",
    "print(f'Cropped Averages:\\n{averages_c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
